{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549a5473-dbe4-4f1a-9c06-06323f467a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/saichandc/qwen/qwen_env_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_interleaved', 'interleaved', 'mrope_section'}\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.60it/s]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "/storage/home/saichandc/qwen/qwen_env_311/lib/python3.11/site-packages/librosa/core/audio.py:172: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<think>\\nGot it, let\\'s see. The image has four luxury cars: Rolls-Royce, Mercedes SUV, Ferrari convertible, Porsche. Then the audio is someone coughing. So need to combine both. The question asks what you see and hear in one short sentence. So: \"Four luxury cars are displayed while someone coughs.\" Let me check. Yes, that\\'s concise.\\n</think>\\n\\nFour luxury cars are displayed while someone coughs.']\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "MODEL_PATH = \"/storage/scratch/saichandc/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "\n",
    "model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"flash_attention_2\", -- commenting this as not using GPU currently\n",
    ")\n",
    "\n",
    "processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n",
    "            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n",
    "            {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one short sentence.\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set whether to use audio in video\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, \n",
    "                   audio=audios, \n",
    "                   images=images, \n",
    "                   videos=videos, \n",
    "                   return_tensors=\"pt\", \n",
    "                   padding=True, \n",
    "                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "# Inference: Generation of the output text and audio\n",
    "text_ids, audio = model.generate(**inputs, \n",
    "                                 speaker=\"Ethan\", \n",
    "                                 thinker_return_dict_in_generate=True,\n",
    "                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "\n",
    "text = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n",
    "                              skip_special_tokens=True,\n",
    "                              clean_up_tokenization_spaces=False)\n",
    "print(text)\n",
    "if audio is not None:\n",
    "    sf.write(\n",
    "        \"output.wav\",\n",
    "        audio.reshape(-1).detach().cpu().numpy(),\n",
    "        samplerate=24000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f005421a-0b73-4a82-8ad3-7228df111304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:40:14\n"
     ]
    }
   ],
   "source": [
    "print(__import__('datetime').datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2982859-6d75-4069-b396-1dbfbfc0ed75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen_env)",
   "language": "python",
   "name": "qwen_env_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
