{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Similarity Analysis\n",
    "This notebook loads embeddings and transcript data, then allows interactive similarity calculations between any two time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration - Update Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THESE PATHS\n",
    "EMBEDDINGS_NPY = \"./second_level_embeddings_first_debate_with_transcript.npy\"\n",
    "#EMBEDDINGS_NPY = \"./video_embeddings_per_second.npy\"\n",
    "TRANSCRIPT_CSV = \"/storage/home/saichandc/qwen/transcript_by_second_first_debate.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings with shape: (2898, 2048)\n",
      "\n",
      "âœ“ Total number of embeddings: 2898\n",
      "âœ“ Embedding dimension: 2048"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "embeds = np.load(EMBEDDINGS_NPY)\n",
    "\n",
    "print(f\"Loaded embeddings with shape: {embeds.shape}\")\n",
    "print(f\"\\nâœ“ Total number of embeddings: {embeds.shape[0]}\")\n",
    "print(f\"âœ“ Embedding dimension: {embeds.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Transcript CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transcript CSV with shape: (2898, 2)\n",
      "Columns: ['second', 'transcript']\n",
      "\n",
      "First few rows:"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>second</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>evening from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the Health Education Campus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Case Western Reserve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   second                   transcript\n",
       "0       0                         Good\n",
       "1       1                 evening from\n",
       "2       2  the Health Education Campus\n",
       "3       3                           of\n",
       "4       4         Case Western Reserve"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load transcript\n",
    "df = pd.read_csv(TRANSCRIPT_CSV)\n",
    "\n",
    "print(f\"Loaded transcript CSV with shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Auto-detect Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time column: 'second'\n",
      "Using text column: 'transcript'"
     ]
    }
   ],
   "source": [
    "# Auto-detect time and text columns\n",
    "time_col = None\n",
    "text_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if col_lower in ['second', 'sec', 'time', 'seconds']:\n",
    "        time_col = col\n",
    "    if col_lower in ['text', 'transcript', 'content', 'speech']:\n",
    "        text_col = col\n",
    "\n",
    "if time_col is None:\n",
    "    time_col = df.columns[0]\n",
    "if text_col is None:\n",
    "    text_col = df.columns[1] if len(df.columns) > 1 else df.columns[0]\n",
    "\n",
    "print(f\"Using time column: '{time_col}'\")\n",
    "print(f\"Using text column: '{text_col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined"
     ]
    }
   ],
   "source": [
    "def get_text_for_second(sec: int) -> str:\n",
    "    \"\"\"Get text for a specific second from transcript\"\"\"\n",
    "    row = df[df[time_col] == sec]\n",
    "    \n",
    "    if len(row) == 0:\n",
    "        return f\"[No text found for second {sec}]\"\n",
    "    \n",
    "    text = row[text_col].iloc[0]\n",
    "    return str(text) if pd.notna(text) else \"[Empty]\"\n",
    "\n",
    "def calculate_similarity(sec1: int, sec2: int) -> float:\n",
    "    \"\"\"Calculate cosine similarity between embeddings at two time points\"\"\"\n",
    "    if sec1 >= len(embeds) or sec2 >= len(embeds):\n",
    "        raise ValueError(f\"Time index out of range. Valid range: 0-{len(embeds)-1}\")\n",
    "    \n",
    "    # Reshape for sklearn\n",
    "    emb1 = embeds[sec1].reshape(1, -1)\n",
    "    emb2 = embeds[sec2].reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(emb1, emb2)[0, 0]\n",
    "    return similarity\n",
    "\n",
    "def compare_seconds(sec1: int, sec2: int):\n",
    "    \"\"\"Compare two seconds and display similarity + text\"\"\"\n",
    "    if sec1 < 0 or sec1 >= len(embeds):\n",
    "        print(f\"ERROR: First second must be between 0 and {len(embeds)-1}\")\n",
    "        return\n",
    "    if sec2 < 0 or sec2 >= len(embeds):\n",
    "        print(f\"ERROR: Second second must be between 0 and {len(embeds)-1}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = calculate_similarity(sec1, sec2)\n",
    "    \n",
    "    # Get texts\n",
    "    text1 = get_text_for_second(sec1)\n",
    "    text2 = get_text_for_second(sec2)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SIMILARITY: Second {sec1} vs Second {sec2}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nðŸ“Š Cosine Similarity: {similarity:.4f}\")\n",
    "    print(f\"\\nðŸ“ Text at second {sec1}:\")\n",
    "    print(f\"   {text1}\")\n",
    "    print(f\"\\nðŸ“ Text at second {sec2}:\")\n",
    "    print(f\"   {text2}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Two Time Points\n",
    "Run this cell and modify `sec1` and `sec2` to compare different time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SIMILARITY: Second 1 vs Second 1446\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Cosine Similarity: 0.8126\n",
      "\n",
      "ðŸ“ Text at second 1:\n",
      "   evening from\n",
      "\n",
      "ðŸ“ Text at second 1446:\n",
      "   & Johnson,\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "# MODIFY THESE VALUES to compare different time points\n",
    "sec1 = 1  # First second\n",
    "sec2 = 1446  # Second second\n",
    "\n",
    "compare_seconds(sec1, sec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Lookups\n",
    "View text at a specific second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second 2897: about here."
     ]
    }
   ],
   "source": [
    "# View text at a specific second\n",
    "sec = 2897  # Change this\n",
    "\n",
    "print(f\"Second {sec}: {get_text_for_second(sec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find Most Similar Second to a Given Time\n",
    "Find which other seconds are most similar to a target second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Top 10 most similar seconds to second 1446\n",
      "======================================================================\n",
      "\n",
      "Target (second 1446): & Johnson,\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1. Second 1445 (similarity: 0.9916)\n",
      "   Moderna, Johnson\n",
      "\n",
      "2. Second 1821 (similarity: 0.9913)\n",
      "   Biden.\n",
      "\n",
      "3. Second 1444 (similarity: 0.9912)\n",
      "   speak to. We have great â€“\n",
      "\n",
      "4. Second 1451 (similarity: 0.9904)\n",
      "   political, because\n",
      "\n",
      "5. Second 1470 (similarity: 0.9898)\n",
      "   the answer\n",
      "\n",
      "6. Second 2689 (similarity: 0.9897)\n",
      "   back manufacturing.\n",
      "\n",
      "7. Second 2379 (similarity: 0.9892)\n",
      "   When? But let me just tell\n",
      "\n",
      "8. Second 1442 (similarity: 0.9892)\n",
      "   to all\n",
      "\n",
      "9. Second 1450 (similarity: 0.9890)\n",
      "   Become very\n",
      "\n",
      "10. Second 2015 (similarity: 0.9889)\n",
      "   to\n",
      "\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "def find_most_similar(target_sec: int, top_n: int = 5, exclude_self: bool = True):\n",
    "    \"\"\"Find the most similar seconds to a target second\"\"\"\n",
    "    if target_sec >= len(embeds):\n",
    "        print(f\"ERROR: Second must be between 0 and {len(embeds)-1}\")\n",
    "        return\n",
    "    \n",
    "    # Get target embedding\n",
    "    target_emb = embeds[target_sec].reshape(1, -1)\n",
    "    \n",
    "    # Calculate similarity with all embeddings\n",
    "    similarities = cosine_similarity(target_emb, embeds)[0]\n",
    "    \n",
    "    # Get top N indices\n",
    "    if exclude_self:\n",
    "        similarities[target_sec] = -1  # Exclude self\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Top {top_n} most similar seconds to second {target_sec}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTarget (second {target_sec}): {get_text_for_second(target_sec)}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        sim = similarities[idx]\n",
    "        text = get_text_for_second(idx)\n",
    "        print(f\"\\n{i}. Second {idx} (similarity: {sim:.4f})\")\n",
    "        print(f\"   {text}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Example usage\n",
    "target_second = 1446  # Change this\n",
    "find_most_similar(target_second, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Top 10 most DISSIMILAR seconds to second 1\n",
      "======================================================================\n",
      "\n",
      "Target (second 1): evening from\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1. Second 816 (similarity: 0.5613)\n",
      "   it. Let him â€“ let him â€“ there is no manifesto.\n",
      "\n",
      "2. Second 1247 (similarity: 0.5693)\n",
      "   minute. You didn't\n",
      "\n",
      "3. Second 1923 (similarity: 0.5768)\n",
      "   one of the last\n",
      "\n",
      "4. Second 2771 (similarity: 0.5824)\n",
      "   [Empty]\n",
      "\n",
      "5. Second 201 (similarity: 0.5887)\n",
      "   candidate. As good as anybody\n",
      "\n",
      "6. Second 2797 (similarity: 0.5918)\n",
      "   him $183\n",
      "\n",
      "7. Second 167 (similarity: 0.5939)\n",
      "   have consequences.\n",
      "\n",
      "8. Second 2516 (similarity: 0.5954)\n",
      "   plan would\n",
      "\n",
      "9. Second 2784 (similarity: 0.6003)\n",
      "   .5 million.\n",
      "\n",
      "10. Second 916 (similarity: 0.6036)\n",
      "   I got you. Premiums\n",
      "\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "def find_most_dissimilar(target_sec: int, top_n: int = 5, exclude_self: bool = True):\n",
    "    \"\"\"Find the most dissimilar seconds to a target second\"\"\"\n",
    "    if target_sec >= len(embeds):\n",
    "        print(f\"ERROR: Second must be between 0 and {len(embeds)-1}\")\n",
    "        return\n",
    "    \n",
    "    # Get target embedding\n",
    "    target_emb = embeds[target_sec].reshape(1, -1)\n",
    "    \n",
    "    # Calculate similarity with all embeddings\n",
    "    similarities = cosine_similarity(target_emb, embeds)[0]\n",
    "    \n",
    "    # Exclude self if requested\n",
    "    if exclude_self:\n",
    "        similarities[target_sec] = 2  # Set to high value to exclude (similarity range is -1 to 1)\n",
    "    \n",
    "    # Get bottom N indices (most dissimilar = lowest similarity)\n",
    "    bottom_indices = np.argsort(similarities)[:top_n]\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Top {top_n} most DISSIMILAR seconds to second {target_sec}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTarget (second {target_sec}): {get_text_for_second(target_sec)}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    for i, idx in enumerate(bottom_indices, 1):\n",
    "        sim = similarities[idx]\n",
    "        text = get_text_for_second(idx)\n",
    "        print(f\"\\n{i}. Second {idx} (similarity: {sim:.4f})\")\n",
    "        print(f\"   {text}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Example usage\n",
    "target_second = 1  # Change this\n",
    "find_most_dissimilar(target_second, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET SUMMARY\n",
      "======================================================================\n",
      "Total embeddings: 2898\n",
      "Embedding dimension: 2048\n",
      "Transcript entries: 2898\n",
      "Valid time range: 0 to 2897 seconds\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total embeddings: {embeds.shape[0]}\")\n",
    "print(f\"Embedding dimension: {embeds.shape[1]}\")\n",
    "print(f\"Transcript entries: {len(df)}\")\n",
    "print(f\"Valid time range: 0 to {embeds.shape[0]-1} seconds\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing local surprise using squared Euclidean distance...\nContext window: 1 second (immediate predecessor)\n\nLoaded embeddings: (2898, 2048)\nEmbedding dimension: 2048\n\nComputing surprise (comparing each second to previous second)...\n\nâœ“ Saved to: /storage/home/saichandc/qwen/surprise_scores_euclidean.csv\n\nSurprise Statistics (raw):\n  Mean: 1586.0371\n  Std:  1198.7875\n  Min:  0.0000\n  Max:  14049.2744\n\n======================================================================\nTOP 10 MOST SURPRISING MOMENTS (Biggest change from previous second)\n======================================================================\n\nSecond 968 (Surprise: 14049.2744)\n  Previous (967): No, no, no, no. But in terms of the court, Justice Barrett,...\n  Current  (968): there...\n\nSecond 2539 (Surprise: 12913.5059)\n  Previous (2538): [Empty]...\n  Current  (2539): it would be about putting out the And number four, it would be about making sure that...\n\nSecond 2666 (Surprise: 10644.5020)\n  Previous (2665): [Empty]...\n  Current  (2666): Wait, wait. Is it fair to say he blew...\n\nSecond 2540 (Surprise: 10206.5137)\n  Previous (2539): it would be about putting out the And number four, it would be about making sure that...\n  Current  (2540): the...\n\nSecond 528 (Surprise: 9595.8398)\n  Previous (527): to be able to shut him up. Mr. President, as a moderator,...\n  Current  (528): we...\n\nSecond 1701 (Surprise: 9188.2344)\n  Previous (1700): provide them plastic â€“ Tell that to Nancy Pelosi....\n  Current  (1701): [Empty]...\n\nSecond 967 (Surprise: 9127.6133)\n  Previous (966): confirm â€“...\n  Current  (967): No, no, no, no. But in terms of the court, Justice Barrett,...\n\nSecond 1973 (Surprise: 8342.4668)\n  Previous (1972): [Empty]...\n  Current  (1973): I think it's fair to say, recovering...\n\nSecond 558 (Surprise: 8326.2559)\n  Previous (557): would you just reply for me? And you don't know her...\n  Current  (558): view on...\n\nSecond 223 (Surprise: 8308.8789)\n  Previous (222): otherwise. And by the way, the Democrats,...\n  Current  (223): they..."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute Local Surprise using Squared Euclidean Distance\n",
    "Window = 1 (compare each second to immediately previous second)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "EMBEDDINGS_NPY = \"/storage/home/saichandc/qwen/second_level_embeddings_first_debate_text_only.npy\"\n",
    "TRANSCRIPT_CSV = \"/storage/home/saichandc/qwen/transcript_by_second_first_debate.csv\"\n",
    "OUTPUT_CSV = \"/storage/home/saichandc/qwen/surprise_scores_euclidean.csv\"\n",
    "\n",
    "CONTEXT_WINDOW = 1  # Compare to immediately previous second\n",
    "\n",
    "# ----------------------------\n",
    "# Compute Local Surprise\n",
    "# ----------------------------\n",
    "def compute_local_surprise_l2_squared(embeds: np.ndarray, window: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Local surprise using L2 norm squared\n",
    "    \n",
    "    Surprise(t) = ||embedding(t) - mean(context)||^2\n",
    "    \n",
    "    With window=1, this is just the squared distance from previous second:\n",
    "    Surprise(t) = ||embedding(t) - embedding(t-1)||^2\n",
    "    \"\"\"\n",
    "    n = len(embeds)\n",
    "    surprise = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            surprise[i] = 0.0  # First second has no previous\n",
    "            continue\n",
    "        \n",
    "        # Get context: previous 'window' seconds\n",
    "        start_idx = max(0, i - window)\n",
    "        context = embeds[start_idx:i]\n",
    "        \n",
    "        # Average of context (with window=1, this is just the previous embedding)\n",
    "        context_mean = context.mean(axis=0)\n",
    "        \n",
    "        # Squared L2 norm of difference\n",
    "        diff = embeds[i] - context_mean\n",
    "        surprise[i] = np.sum(diff ** 2)  # ||diff||^2\n",
    "    \n",
    "    return surprise\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    print(\"Computing local surprise using squared Euclidean distance...\")\n",
    "    print(f\"Context window: {CONTEXT_WINDOW} second (immediate predecessor)\\n\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeds = np.load(EMBEDDINGS_NPY)\n",
    "    print(f\"Loaded embeddings: {embeds.shape}\")\n",
    "    print(f\"Embedding dimension: {embeds.shape[1]}\")\n",
    "    \n",
    "    # Load transcript\n",
    "    df = pd.read_csv(TRANSCRIPT_CSV)\n",
    "    \n",
    "    # Auto-detect columns\n",
    "    time_col = next((col for col in df.columns if col.lower() in ['second', 'sec', 'time', 'seconds']), df.columns[0])\n",
    "    text_col = next((col for col in df.columns if col.lower() in ['text', 'transcript', 'content', 'speech']), df.columns[1])\n",
    "    \n",
    "    # Compute surprise\n",
    "    print(f\"\\nComputing surprise (comparing each second to previous second)...\")\n",
    "    surprise = compute_local_surprise_l2_squared(embeds, window=CONTEXT_WINDOW)\n",
    "    \n",
    "    # Create results\n",
    "    results = pd.DataFrame({\n",
    "        'second': df[time_col][:len(embeds)],\n",
    "        'text': df[text_col][:len(embeds)],\n",
    "        'surprise_raw': surprise\n",
    "    })\n",
    "    \n",
    "    # Save\n",
    "    results.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\nâœ“ Saved to: {OUTPUT_CSV}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nSurprise Statistics (raw):\")\n",
    "    print(f\"  Mean: {surprise.mean():.4f}\")\n",
    "    print(f\"  Std:  {surprise.std():.4f}\")\n",
    "    print(f\"  Min:  {surprise.min():.4f}\")\n",
    "    print(f\"  Max:  {surprise.max():.4f}\")\n",
    "    \n",
    "    # Top 10 most surprising (biggest jumps from previous second)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TOP 10 MOST SURPRISING MOMENTS (Biggest change from previous second)\")\n",
    "    print('='*70)\n",
    "    top_10 = results.nlargest(10, 'surprise_raw')\n",
    "    for _, row in top_10.iterrows():\n",
    "        sec = int(row['second'])\n",
    "        print(f\"\\nSecond {sec} (Surprise: {row['surprise_raw']:.4f})\")\n",
    "        \n",
    "        # Show current and previous text\n",
    "        if sec > 0:\n",
    "            # Get previous text safely\n",
    "            prev_row = results[results['second'] == sec - 1]\n",
    "            if len(prev_row) > 0:\n",
    "                prev_text = prev_row['text'].values[0]\n",
    "                # Handle NaN or non-string values\n",
    "                if pd.isna(prev_text):\n",
    "                    prev_text = \"[Empty]\"\n",
    "                else:\n",
    "                    prev_text = str(prev_text)\n",
    "            else:\n",
    "                prev_text = \"[N/A]\"\n",
    "            \n",
    "            # Get current text safely\n",
    "            curr_text = row['text']\n",
    "            if pd.isna(curr_text):\n",
    "                curr_text = \"[Empty]\"\n",
    "            else:\n",
    "                curr_text = str(curr_text)\n",
    "            \n",
    "            print(f\"  Previous ({sec-1}): {prev_text[:100]}...\")\n",
    "            print(f\"  Current  ({sec}): {curr_text[:100]}...\")\n",
    "        else:\n",
    "            curr_text = row['text']\n",
    "            if pd.isna(curr_text):\n",
    "                curr_text = \"[Empty]\"\n",
    "            else:\n",
    "                curr_text = str(curr_text)\n",
    "            print(f\"  {curr_text[:100]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing semantic speed from text embeddings...\nTime base = 1 second\n  distance(t) = ||x_{t+1} - x_t|| (embedding distance units)\n  speed(t) = distance(t) / 1 sec (embedding distance/second)\n  â†’ Numerically identical, but different units\n\nLoaded embeddings: (2898, 2048)\nEmbedding dimension: 2048\n\nComputing distances/speeds between consecutive seconds...\n\n======================================================================\nAVERAGE SPEED\n======================================================================\nAverage speed = mean(distance(t)) = 37.740711 units/second\nTotal distance traveled = 109334.841009 units\n======================================================================\n\nâœ“ Saved to: /storage/home/saichandc/qwen/speed_scores.csv\n\nSpeed/Distance Statistics:\n  Mean:   37.727688 units/sec\n  Std:    12.753768 units/sec\n  Min:    0.000000 units/sec\n  Max:    118.529633 units/sec\n  Median: 35.260599 units/sec\n\n======================================================================\nTOP 10 HIGHEST SEMANTIC SPEEDS (Largest jumps)\n======================================================================\n\nSecond 967 â†’ 968 (Speed: 118.529633 units/sec)\n  From (967): No, no, no, no. But in terms of the court, Justice Barrett,...\n  To   (968): there...\n\nSecond 2538 â†’ 2539 (Speed: 113.637611 units/sec)\n  From (2538): [Empty]...\n  To   (2539): it would be about putting out the And number four, it would be about making sure that...\n\nSecond 2665 â†’ 2666 (Speed: 103.172195 units/sec)\n  From (2665): [Empty]...\n  To   (2666): Wait, wait. Is it fair to say he blew...\n\nSecond 2539 â†’ 2540 (Speed: 101.027290 units/sec)\n  From (2539): it would be about putting out the And number four, it would be about making sure that...\n  To   (2540): the...\n\nSecond 527 â†’ 528 (Speed: 97.958359 units/sec)\n  From (527): to be able to shut him up. Mr. President, as a moderator,...\n  To   (528): we...\n\nSecond 1700 â†’ 1701 (Speed: 95.855278 units/sec)\n  From (1700): provide them plastic â€“ Tell that to Nancy Pelosi....\n  To   (1701): [Empty]...\n\nSecond 966 â†’ 967 (Speed: 95.538544 units/sec)\n  From (966): confirm â€“...\n  To   (967): No, no, no, no. But in terms of the court, Justice Barrett,...\n\nSecond 1972 â†’ 1973 (Speed: 91.337105 units/sec)\n  From (1972): [Empty]...\n  To   (1973): I think it's fair to say, recovering...\n\nSecond 557 â†’ 558 (Speed: 91.248322 units/sec)\n  From (557): would you just reply for me? And you don't know her...\n  To   (558): view on...\n\nSecond 222 â†’ 223 (Speed: 91.153053 units/sec)\n  From (222): otherwise. And by the way, the Democrats,...\n  To   (223): they...\n\n======================================================================\nSUMMARY\n======================================================================\nTotal seconds analyzed: 2898\nAverage semantic speed: 37.740711 units/sec\nTotal semantic distance: 109334.841009 units\n\nNote: With time base = 1 second:\n  - distance and speed are numerically identical\n  - distance: spatial units in embedding space\n  - speed: spatial units per second\n======================================================================"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute Speed: Average Euclidean distance traveled between consecutive embeddings\n",
    "Time base = 1 second (so distance values = speed at each timestep)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "EMBEDDINGS_NPY = \"/storage/home/saichandc/qwen/second_level_embeddings_first_debate_text_only.npy\"\n",
    "TRANSCRIPT_CSV = \"/storage/home/saichandc/qwen/transcript_by_second_first_debate.csv\"\n",
    "OUTPUT_CSV = \"/storage/home/saichandc/qwen/speed_scores.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# Compute Distance and Speed\n",
    "# ----------------------------\n",
    "def compute_distance_and_speed(embeds: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between consecutive embeddings\n",
    "    \n",
    "    With time base = 1 second:\n",
    "    - distance(t) = ||x_{t+1} - x_t|| (units: embedding distance / second)\n",
    "    - speed(t) = distance(t) / 1 second = distance(t) (numerically same)\n",
    "    - average_speed = mean(speed(t)) = mean(distance(t))\n",
    "    \n",
    "    Returns:\n",
    "        distances: array of distances for each timestep (= instantaneous speed)\n",
    "        avg_speed: average speed (scalar)\n",
    "    \"\"\"\n",
    "    n = len(embeds)\n",
    "    distances = np.zeros(n)\n",
    "    \n",
    "    for i in range(n - 1):\n",
    "        # Euclidean distance (NOT squared)\n",
    "        diff = embeds[i+1] - embeds[i]\n",
    "        distances[i] = np.sqrt(np.sum(diff ** 2))  # ||x_{t+1} - x_t||\n",
    "    \n",
    "    # Last timestep has no next point\n",
    "    distances[-1] = 0.0\n",
    "    \n",
    "    # Average speed = mean of all instantaneous speeds\n",
    "    avg_speed = distances[:-1].mean() if n > 1 else 0.0\n",
    "    \n",
    "    return distances, avg_speed\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    print(\"Computing semantic speed from text embeddings...\")\n",
    "    print(\"Time base = 1 second\")\n",
    "    print(\"  distance(t) = ||x_{t+1} - x_t|| (embedding distance units)\")\n",
    "    print(\"  speed(t) = distance(t) / 1 sec (embedding distance/second)\")\n",
    "    print(\"  â†’ Numerically identical, but different units\\n\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeds = np.load(EMBEDDINGS_NPY)\n",
    "    print(f\"Loaded embeddings: {embeds.shape}\")\n",
    "    print(f\"Embedding dimension: {embeds.shape[1]}\")\n",
    "    \n",
    "    # Load transcript\n",
    "    df = pd.read_csv(TRANSCRIPT_CSV)\n",
    "    \n",
    "    # Auto-detect columns\n",
    "    time_col = next((col for col in df.columns if col.lower() in ['second', 'sec', 'time', 'seconds']), df.columns[0])\n",
    "    text_col = next((col for col in df.columns if col.lower() in ['text', 'transcript', 'content', 'speech']), df.columns[1])\n",
    "    \n",
    "    # Compute distances and speed\n",
    "    print(f\"\\nComputing distances/speeds between consecutive seconds...\")\n",
    "    distances, avg_speed = compute_distance_and_speed(embeds)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"AVERAGE SPEED\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Average speed = mean(distance(t)) = {avg_speed:.6f} units/second\")\n",
    "    print(f\"Total distance traveled = {distances.sum():.6f} units\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create results\n",
    "    # Note: distance and speed are numerically the same with time base = 1\n",
    "    results = pd.DataFrame({\n",
    "        'second': df[time_col][:len(embeds)],\n",
    "        'text': df[text_col][:len(embeds)],\n",
    "        'distance': distances,  # distance traveled\n",
    "        'speed': distances      # speed = distance/1sec (numerically same)\n",
    "    })\n",
    "    \n",
    "    # Save\n",
    "    results.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\nâœ“ Saved to: {OUTPUT_CSV}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nSpeed/Distance Statistics:\")\n",
    "    print(f\"  Mean:   {distances.mean():.6f} units/sec\")\n",
    "    print(f\"  Std:    {distances.std():.6f} units/sec\")\n",
    "    print(f\"  Min:    {distances.min():.6f} units/sec\")\n",
    "    print(f\"  Max:    {distances.max():.6f} units/sec\")\n",
    "    print(f\"  Median: {np.median(distances):.6f} units/sec\")\n",
    "    \n",
    "    # Top 10 highest speeds (largest semantic jumps)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TOP 10 HIGHEST SEMANTIC SPEEDS (Largest jumps)\")\n",
    "    print('='*70)\n",
    "    top_10 = results.nlargest(10, 'speed')\n",
    "    for _, row in top_10.iterrows():\n",
    "        sec = int(row['second'])\n",
    "        print(f\"\\nSecond {sec} â†’ {sec+1} (Speed: {row['speed']:.6f} units/sec)\")\n",
    "        \n",
    "        # Show current and next text\n",
    "        curr_text = row['text']\n",
    "        if pd.isna(curr_text):\n",
    "            curr_text = \"[Empty]\"\n",
    "        else:\n",
    "            curr_text = str(curr_text)\n",
    "        \n",
    "        # Get next text\n",
    "        if sec < len(results) - 1:\n",
    "            next_row = results[results['second'] == sec + 1]\n",
    "            if len(next_row) > 0:\n",
    "                next_text = next_row['text'].values[0]\n",
    "                if pd.isna(next_text):\n",
    "                    next_text = \"[Empty]\"\n",
    "                else:\n",
    "                    next_text = str(next_text)\n",
    "            else:\n",
    "                next_text = \"[N/A]\"\n",
    "            \n",
    "            print(f\"  From ({sec}): {curr_text[:100]}...\")\n",
    "            print(f\"  To   ({sec+1}): {next_text[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  Current ({sec}): {curr_text[:100]}...\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print('='*70)\n",
    "    print(f\"Total seconds analyzed: {len(embeds)}\")\n",
    "    print(f\"Average semantic speed: {avg_speed:.6f} units/sec\")\n",
    "    print(f\"Total semantic distance: {distances.sum():.6f} units\")\n",
    "    print(f\"\\nNote: With time base = 1 second:\")\n",
    "    print(f\"  - distance and speed are numerically identical\")\n",
    "    print(f\"  - distance: spatial units in embedding space\")\n",
    "    print(f\"  - speed: spatial units per second\")\n",
    "    print('='*70)\n",
    "    \n",
    "    return results, avg_speed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, speed = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
