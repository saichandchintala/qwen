{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a5473-dbe4-4f1a-9c06-06323f467a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "MODEL_PATH = \"/storage/scratch/saichandc/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "\n",
    "model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"flash_attention_2\", -- commenting this as not using GPU currently\n",
    ")\n",
    "\n",
    "processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n",
    "            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n",
    "            {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one short sentence.\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set whether to use audio in video\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, \n",
    "                   audio=audios, \n",
    "                   images=images, \n",
    "                   videos=videos, \n",
    "                   return_tensors=\"pt\", \n",
    "                   padding=True, \n",
    "                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "# Inference: Generation of the output text and audio\n",
    "text_ids, audio = model.generate(**inputs, \n",
    "                                 speaker=\"Ethan\", \n",
    "                                 thinker_return_dict_in_generate=True,\n",
    "                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "\n",
    "text = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n",
    "                              skip_special_tokens=True,\n",
    "                              clean_up_tokenization_spaces=False)\n",
    "print(text)\n",
    "if audio is not None:\n",
    "    sf.write(\n",
    "        \"output.wav\",\n",
    "        audio.reshape(-1).detach().cpu().numpy(),\n",
    "        samplerate=24000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005421a-0b73-4a82-8ad3-7228df111304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__import__('datetime').datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2982859-6d75-4069-b396-1dbfbfc0ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n",
    "# from qwen_omni_utils import process_mm_info\n",
    "# from tqdm import tqdm\n",
    "# import subprocess\n",
    "\n",
    "# MODEL_PATH = \"/storage/scratch/saichandc/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "# VIDEO_PATH = \"/storage/home/saichandc/video/90secvideo.mp4\"\n",
    "# USE_AUDIO_IN_VIDEO = True\n",
    "# FIXED_TEXT = \"Commission on Presidential\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Working code - tested on 90 sec video. \n",
    "Mini-run (2 seconds) → per-second multimodal embeddings with Qwen3-Omni (MoviePy v2)\n",
    "\n",
    "Key fixes:\n",
    "- Use absolute paths for segments\n",
    "- Convert segment paths to proper file URIs: Path(...).resolve().as_uri()\n",
    "- Remove 'verbose'/'logger' args from write_videofile (MoviePy v2)\n",
    "- Talker disabled + USE_AUDIO_IN_VIDEO=True\n",
    "- Error handling + tqdm progress bars\n",
    "\n",
    "Refs:\n",
    "- MoviePy v2 import & examples: https://pypi.org/project/moviepy/            # top-level import, v2 API\n",
    "- qwen-omni-utils file path/URI usage: https://pypi.org/project/qwen-omni-utils/  # 'file:///path/to/...'\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from moviepy import VideoFileClip  # MoviePy v2 import (not moviepy.editor)\n",
    "\n",
    "from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "MODEL_PATH = \"/storage/scratch/saichandc/Qwen3-Omni-30B-A3B-Thinking\"   # or \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "VIDEO_PATH = \"/storage/home/saichandc/video/90secvideo.mp4\"                                  # <-- change to your video path\n",
    "OUT_DIR = \"./tmp_slices_v2\"\n",
    "SAVE_EMBEDDINGS_NPY = \"./second_level_embeddings.npy\"\n",
    "\n",
    "PLACEHOLDER_TEXT = \"Process this 1-second slice.\"\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "BATCH_SIZE = 2\n",
    "MAX_SECS = None     # only first 2 seconds now; set None for full length later\n",
    "\n",
    "LOG_LEVEL = logging.INFO\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_DIR = str(Path(OUT_DIR).resolve())  # <-- absolutize output dir\n",
    "\n",
    "logging.basicConfig(level=LOG_LEVEL, format=\"[%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"qwen_embeddings\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: masked mean pooling\n",
    "# ----------------------------\n",
    "def pooled_last_hidden_state(last_hidden: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    mask = attention_mask.unsqueeze(-1).float()  # [B, L, 1]\n",
    "    summed = (last_hidden * mask).sum(dim=1)     # [B, H]\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-6)      # [B, 1]\n",
    "    return summed / denom\n",
    "\n",
    "# ----------------------------\n",
    "# Load Qwen model & processor\n",
    "# ----------------------------\n",
    "def load_qwen():\n",
    "    try:\n",
    "        model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n",
    "            MODEL_PATH, dtype=\"auto\", device_map=\"auto\", trust_remote_code=True,\n",
    "        )\n",
    "        processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True,)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Failed to load Qwen model/processor.\")\n",
    "        raise SystemExit(e)\n",
    "\n",
    "    # Save ~2GB if you don't need audio outputs\n",
    "    try:\n",
    "        model.disable_talker()\n",
    "        logger.info(\"Talker disabled (no audio generation).\")\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Could not disable Talker explicitly; continuing without audio generation. Details: %s\", e)\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "# ----------------------------\n",
    "# Slice video → 1-second MP4 segments (MoviePy v2)\n",
    "# ----------------------------\n",
    "def slice_video_v2(video_path: str, out_dir: str, max_secs: int | None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Export 1-second segments with MoviePy v2 `.subclipped`.\n",
    "    Return [{\"sec\": t, \"video_uri\": file:///..., \"video_path\": /abs/path.mp4}, ...]\n",
    "    \"\"\"\n",
    "    vp = Path(video_path).resolve()\n",
    "    if not vp.exists():\n",
    "        raise FileNotFoundError(f\"Video not found: {vp}\")\n",
    "\n",
    "    try:\n",
    "        clip = VideoFileClip(str(vp))\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Failed to open video with MoviePy v2.\")\n",
    "        raise\n",
    "\n",
    "    duration = float(clip.duration or 0.0)\n",
    "    total_secs = math.ceil(duration)\n",
    "\n",
    "    if max_secs is None:\n",
    "        n_secs = total_secs\n",
    "        logger.info(\"Processing FULL length: %d second(s).\", n_secs)\n",
    "    else:\n",
    "        n_secs = min(max_secs, total_secs)\n",
    "        logger.info(\"Processing FIRST %d second(s) (of %d).\", n_secs, total_secs)\n",
    "\n",
    "    items: List[Dict] = []\n",
    "    logger.info(\"Slicing %s → %d 1-second segment(s)...\", vp.name, n_secs)\n",
    "\n",
    "    fps = getattr(clip, \"fps\", None)\n",
    "\n",
    "    for t in tqdm(range(n_secs), desc=\"Slicing seconds\", unit=\"s\"):\n",
    "        start, end = t, min(t + 1, duration)\n",
    "        if end <= start:\n",
    "            continue\n",
    "        try:\n",
    "            sub = clip.subclipped(start, end)  # MoviePy v2 API\n",
    "            out_file = Path(out_dir) / f\"seg_{t:06d}.mp4\"\n",
    "\n",
    "            # IMPORTANT: write without 'verbose'/'logger' (v2); keep audio\n",
    "            if fps is not None:\n",
    "                sub.write_videofile(str(out_file), fps=fps, audio=True, codec=\"libx264\", audio_codec=\"aac\")\n",
    "            else:\n",
    "                sub.write_videofile(str(out_file), audio=True, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "            sub.close()\n",
    "\n",
    "            # Build proper absolute path + file URI\n",
    "            abs_path = out_file.resolve()\n",
    "            items.append({\n",
    "                \"sec\": t,\n",
    "                \"video_path\": str(abs_path),\n",
    "                \"video_uri\": abs_path.as_uri(),  # e.g., file:///storage/home/...\n",
    "            })\n",
    "        except Exception as se:\n",
    "            logger.error(\"Failed to write segment %d: %s\", t, se)\n",
    "            continue\n",
    "\n",
    "    clip.close()\n",
    "    if not items:\n",
    "        raise RuntimeError(\"No segments produced. Check the video and codecs.\")\n",
    "    logger.info(\"Created %d segment(s) in %s\", len(items), out_dir)\n",
    "    return items\n",
    "\n",
    "# ----------------------------\n",
    "# Build conversations & extract embeddings\n",
    "# ----------------------------\n",
    "def extract_second_level_embeddings(model, processor, sec_items: List[Dict]) -> np.ndarray:\n",
    "    failed_seconds = []\n",
    "    embeds_all = []\n",
    "\n",
    "    logger.info(\"Building per-second conversations and extracting embeddings...\")\n",
    "    for i in tqdm(range(0, len(sec_items), BATCH_SIZE), desc=\"Embedding batches\", unit=\"batch\"):\n",
    "        batch = sec_items[i:i + BATCH_SIZE]\n",
    "\n",
    "        conversations = []\n",
    "        for it in batch:\n",
    "            # You can use either 'video_uri' (file:///...) or 'video_path' (plain absolute path).\n",
    "            # Both are accepted by qwen-omni-utils (docs show 'file:///...' explicitly).  # see PyPI page\n",
    "            conversations.append([\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"video\", \"video\": it[\"video_uri\"]},\n",
    "                        {\"type\": \"text\",  \"text\": PLACEHOLDER_TEXT},\n",
    "                    ],\n",
    "                }\n",
    "            ])\n",
    "\n",
    "        try:\n",
    "            text = processor.apply_chat_template(\n",
    "                conversations, add_generation_prompt=False, tokenize=False\n",
    "            )\n",
    "            audios, images, videos = process_mm_info(\n",
    "                conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "            )\n",
    "            inputs = processor(\n",
    "                text=text,\n",
    "                audio=audios,\n",
    "                images=images,\n",
    "                videos=videos,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                use_audio_in_video=USE_AUDIO_IN_VIDEO,\n",
    "            ).to(model.device).to(model.dtype)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Processor/prep failed for batch starting at sec %d: %s\", batch[0][\"sec\"], e)\n",
    "            # Helpful debug: print the exact URI/path we passed\n",
    "            for dbg in batch:\n",
    "                logger.error(\"   video_uri=%s | exists=%s\",\n",
    "                             dbg[\"video_uri\"], Path(dbg[\"video_path\"]).exists())\n",
    "            failed_seconds.extend([it[\"sec\"] for it in batch])\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # forward pass using the processor to match multimodal input spec\n",
    "                outputs = model.thinker(**inputs,output_hidden_states=True,return_dict=True)\n",
    "                last_hidden = outputs.hidden_states[-1]\n",
    "                pooled = pooled_last_hidden_state(last_hidden, inputs[\"attention_mask\"])\n",
    "\n",
    "                embeds_all.append(pooled.detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            logger.error(\"Model forward failed for batch starting at sec %d: %s\", batch[0][\"sec\"], e)\n",
    "            failed_seconds.extend([it[\"sec\"] for it in batch])\n",
    "            continue\n",
    "\n",
    "    if not embeds_all:\n",
    "        raise RuntimeError(\"No embeddings produced; all batches failed.\")\n",
    "\n",
    "    embeds = np.concatenate(embeds_all, axis=0)\n",
    "    if failed_seconds:\n",
    "        logger.warning(\"Failed seconds (skipped): %s\", failed_seconds)\n",
    "    else:\n",
    "        logger.info(\"All seconds processed successfully.\")\n",
    "    return embeds\n",
    "\n",
    "# ----------------------------\n",
    "# Entry point\n",
    "# ----------------------------\n",
    "def main():\n",
    "    start_total = time.time()\n",
    "\n",
    "    # ---- Load model ----\n",
    "    t0 = time.time()\n",
    "    model, processor = load_qwen()\n",
    "    logger.info(\"Model + processor loaded in %.2f sec\", time.time() - t0)\n",
    "\n",
    "    # ---- Slice video ----\n",
    "    t1 = time.time()\n",
    "    try:\n",
    "        sec_items = slice_video_v2(VIDEO_PATH, OUT_DIR, max_secs=MAX_SECS)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Slicing failed.\")\n",
    "        raise SystemExit(e)\n",
    "    logger.info(\"Video slicing completed in %.2f sec\", time.time() - t1)\n",
    "\n",
    "    # ---- Extract embeddings ----\n",
    "    t2 = time.time()\n",
    "    try:\n",
    "        embeds = extract_second_level_embeddings(model, processor, sec_items)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Embedding extraction failed.\")\n",
    "        raise SystemExit(e)\n",
    "    logger.info(\"Embedding extraction completed in %.2f sec\", time.time() - t2)\n",
    "\n",
    "    # ---- Save embeddings ----\n",
    "    np.save(SAVE_EMBEDDINGS_NPY, embeds)\n",
    "    logger.info(\"Saved embeddings: shape=%s -> %s\", embeds.shape, SAVE_EMBEDDINGS_NPY)\n",
    "\n",
    "    # ---- Total time ----\n",
    "    logger.info(\"Total runtime: %.2f sec (%.2f min)\", time.time() - start_total, (time.time() - start_total)/60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad76b6-fc87-4588-b868-a9116b615370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Working code - tested on 90 sec video.\n",
    "Per-second multimodal embeddings with Qwen3-Omni.\n",
    "Now slices via FFmpeg CLI for reliability (audio intact), with timing.\n",
    "\n",
    "Key fixes:\n",
    "- Use FFmpeg CLI to slice 1-second segments (keeps audio reliably).\n",
    "- Absolute paths + proper file URIs (Path(...).resolve().as_uri()).\n",
    "- Talker disabled + USE_AUDIO_IN_VIDEO=True.\n",
    "- Error handling + tqdm progress bars.\n",
    "- Step timing logs added.\n",
    "\n",
    "Refs:\n",
    "- MoviePy v2 import & examples: https://pypi.org/project/moviepy/\n",
    "- qwen-omni-utils file path/URI usage: https://pypi.org/project/qwen-omni-utils/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import time\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# MoviePy is only used as a fallback to read duration if ffprobe is unavailable.\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "MODEL_PATH = \"/storage/scratch/saichandc/Qwen3-Omni-30B-A3B-Thinking\"   # or \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n",
    "VIDEO_PATH = \"/storage/home/saichandc/video/90secvideo.mp4\"             # <-- change to your video path\n",
    "OUT_DIR = \"./tmp_slices_v2\"\n",
    "SAVE_EMBEDDINGS_NPY = \"./second_level_embeddings.npy\"\n",
    "\n",
    "PLACEHOLDER_TEXT = \"Process this 1-second slice.\"\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "BATCH_SIZE = 2\n",
    "MAX_SECS = None   # None for full length; set an int for a subset\n",
    "PREFER_FFMPEG_SLICER = True  # set False to use MoviePy slicer (not recommended for audio)\n",
    "\n",
    "LOG_LEVEL = logging.INFO\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_DIR = str(Path(OUT_DIR).resolve())  # absolutize output dir\n",
    "\n",
    "logging.basicConfig(level=LOG_LEVEL, format=\"[%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"qwen_embeddings\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: masked mean pooling\n",
    "# ----------------------------\n",
    "def pooled_last_hidden_state(last_hidden: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    mask = attention_mask.unsqueeze(-1).float()  # [B, L, 1]\n",
    "    summed = (last_hidden * mask).sum(dim=1)     # [B, H]\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-6)      # [B, 1]\n",
    "    return summed / denom\n",
    "\n",
    "# ----------------------------\n",
    "# Load Qwen model & processor\n",
    "# ----------------------------\n",
    "def load_qwen():\n",
    "    try:\n",
    "        model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n",
    "            MODEL_PATH, dtype=\"auto\", device_map=\"auto\", trust_remote_code=True,\n",
    "        )\n",
    "        processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True,)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Failed to load Qwen model/processor.\")\n",
    "        raise SystemExit(e)\n",
    "\n",
    "    # Save ~2GB if you don't need audio outputs\n",
    "    try:\n",
    "        model.disable_talker()\n",
    "        logger.info(\"Talker disabled (no audio generation).\")\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Could not disable Talker explicitly; continuing without audio generation. Details: %s\", e)\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "# ----------------------------\n",
    "# FFmpeg helpers\n",
    "# ----------------------------\n",
    "def ffmpeg_available() -> bool:\n",
    "    \"\"\"Return True if ffmpeg is available.\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def probe_duration_ffprobe(vp: Path) -> float:\n",
    "    \"\"\"Probe video duration using ffprobe; fallback to MoviePy if unavailable.\"\"\"\n",
    "    cmd = f'ffprobe -v error -show_entries format=duration -of default=nw=1:nk=1 \"{vp}\"'\n",
    "    try:\n",
    "        res = subprocess.run(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True)\n",
    "        return float(res.stdout.strip())\n",
    "    except Exception:\n",
    "        clip = VideoFileClip(str(vp))\n",
    "        duration = float(clip.duration or 0.0)\n",
    "        clip.close()\n",
    "        return duration\n",
    "\n",
    "# ----------------------------\n",
    "# Slice video → 1-second MP4 segments (FFmpeg CLI; robust for audio)\n",
    "# ----------------------------\n",
    "def slice_video_ffmpeg(video_path: str, out_dir: str, max_secs: int | None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Export 1-second segments using FFmpeg CLI (robust + keeps audio).\n",
    "    Return [{\"sec\": t, \"video_uri\": file:///..., \"video_path\": /abs/path.mp4, \"slice_time_sec\": float}, ...]\n",
    "    \"\"\"\n",
    "    vp = Path(video_path).resolve()\n",
    "    if not vp.exists():\n",
    "        raise FileNotFoundError(f\"Video not found: {vp}\")\n",
    "\n",
    "    duration = probe_duration_ffprobe(vp)\n",
    "    total_secs = math.ceil(duration)\n",
    "    n_secs = total_secs if max_secs is None else min(max_secs, total_secs)\n",
    "    logger.info(\"FFmpeg slicing %s → %d second(s) (of %d).\", vp.name, n_secs, total_secs)\n",
    "\n",
    "    items: List[Dict] = []\n",
    "    for t in tqdm(range(n_secs), desc=\"Slicing seconds\", unit=\"s\"):\n",
    "        seg_start = time.time()\n",
    "        start, end = t, min(t + 1, duration)\n",
    "        if end <= start:\n",
    "            continue\n",
    "\n",
    "        out_file = Path(out_dir) / f\"seg_{t:06d}.mp4\"\n",
    "\n",
    "        # Resume-friendly: skip if already exists and non-empty\n",
    "        if out_file.exists() and out_file.stat().st_size > 0:\n",
    "            abs_path = out_file.resolve()\n",
    "            items.append({\n",
    "                \"sec\": t,\n",
    "                \"video_path\": str(abs_path),\n",
    "                \"video_uri\": abs_path.as_uri(),\n",
    "                \"slice_time_sec\": 0.0,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Use AAC audio + H.264 video; map first audio stream optionally (if present)\n",
    "        cmd = (\n",
    "            f'ffmpeg -hide_banner -nostdin -y -loglevel error '\n",
    "            f'-ss {start:.3f} -to {end:.3f} -i \"{vp}\" '\n",
    "            f'-map 0:v:0 -map 0:a:0? '\n",
    "            f'-c:v libx264 -preset veryfast -crf 23 '\n",
    "            f'-c:a aac -b:a 128k '\n",
    "            f'\"{out_file}\"'\n",
    "        )\n",
    "        try:\n",
    "            subprocess.run(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            err = e.stderr if isinstance(e.stderr, str) else e.stderr.decode(errors=\"ignore\")\n",
    "            snippet = \"\\n\".join(err.splitlines()[-10:])\n",
    "            logger.error(\"FFmpeg failed at segment %d (rc=%s). Last lines:\\n%s\", t, e.returncode, snippet)\n",
    "            continue\n",
    "\n",
    "        abs_path = out_file.resolve()\n",
    "        items.append({\n",
    "            \"sec\": t,\n",
    "            \"video_path\": str(abs_path),\n",
    "            \"video_uri\": abs_path.as_uri(),  # e.g., file:///storage/home/...\n",
    "            \"slice_time_sec\": round(time.time() - seg_start, 3),\n",
    "        })\n",
    "        logger.info(\"FFmpeg segment %06d written in %.2f sec\", t, time.time() - seg_start)\n",
    "\n",
    "    if not items:\n",
    "        raise RuntimeError(\"No segments produced by FFmpeg. Check installation/codecs.\")\n",
    "    logger.info(\"FFmpeg created %d segment(s) in %s\", len(items), out_dir)\n",
    "    return items\n",
    "\n",
    "# ----------------------------\n",
    "# (Original) Slice video with MoviePy v2 (kept for fallback; not recommended for audio)\n",
    "# ----------------------------\n",
    "def slice_video_v2(video_path: str, out_dir: str, max_secs: int | None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Export 1-second segments with MoviePy v2 `.subclipped`.\n",
    "    Return [{\"sec\": t, \"video_uri\": file:///..., \"video_path\": /abs/path.mp4}, ...]\n",
    "    \"\"\"\n",
    "    vp = Path(video_path).resolve()\n",
    "    if not vp.exists():\n",
    "        raise FileNotFoundError(f\"Video not found: {vp}\")\n",
    "\n",
    "    try:\n",
    "        clip = VideoFileClip(str(vp))\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Failed to open video with MoviePy v2.\")\n",
    "        raise\n",
    "\n",
    "    duration = float(clip.duration or 0.0)\n",
    "    total_secs = math.ceil(duration)\n",
    "    n_secs = total_secs if max_secs is None else min(max_secs, total_secs)\n",
    "    logger.info(\"Processing %s → %d 1-second segment(s) (of %d).\", vp.name, n_secs, total_secs)\n",
    "\n",
    "    items: List[Dict] = []\n",
    "    fps = getattr(clip, \"fps\", None)\n",
    "\n",
    "    for t in tqdm(range(n_secs), desc=\"Slicing seconds\", unit=\"s\"):\n",
    "        start, end = t, min(t + 1, duration)\n",
    "        if end <= start:\n",
    "            continue\n",
    "        try:\n",
    "            sub = clip.subclipped(start, end)  # MoviePy v2 API\n",
    "            out_file = Path(out_dir) / f\"seg_{t:06d}.mp4\"\n",
    "\n",
    "            # IMPORTANT: write without 'verbose'/'logger' (v2); keep audio\n",
    "            if fps is not None:\n",
    "                sub.write_videofile(str(out_file), fps=fps, audio=True, codec=\"libx264\", audio_codec=\"aac\")\n",
    "            else:\n",
    "                sub.write_videofile(str(out_file), audio=True, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "            sub.close()\n",
    "\n",
    "            abs_path = out_file.resolve()\n",
    "            items.append({\n",
    "                \"sec\": t,\n",
    "                \"video_path\": str(abs_path),\n",
    "                \"video_uri\": abs_path.as_uri(),\n",
    "            })\n",
    "        except Exception as se:\n",
    "            logger.error(\"Failed to write segment %d: %s\", t, se)\n",
    "            continue\n",
    "\n",
    "    clip.close()\n",
    "    if not items:\n",
    "        raise RuntimeError(\"No segments produced. Check the video and codecs.\")\n",
    "    logger.info(\"Created %d segment(s) in %s\", len(items), out_dir)\n",
    "    return items\n",
    "\n",
    "# ----------------------------\n",
    "# Build conversations & extract embeddings\n",
    "# ----------------------------\n",
    "def extract_second_level_embeddings(model, processor, sec_items: List[Dict]) -> np.ndarray:\n",
    "    failed_seconds = []\n",
    "    embeds_all = []\n",
    "\n",
    "    logger.info(\"Building per-second conversations and extracting embeddings...\")\n",
    "    for i in tqdm(range(0, len(sec_items), BATCH_SIZE), desc=\"Embedding batches\", unit=\"batch\"):\n",
    "        batch = sec_items[i:i + BATCH_SIZE]\n",
    "\n",
    "        conversations = []\n",
    "        for it in batch:\n",
    "            conversations.append([\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"video\", \"video\": it[\"video_uri\"]},\n",
    "                        {\"type\": \"text\",  \"text\": PLACEHOLDER_TEXT},\n",
    "                    ],\n",
    "                }\n",
    "            ])\n",
    "\n",
    "        try:\n",
    "            text = processor.apply_chat_template(\n",
    "                conversations, add_generation_prompt=False, tokenize=False\n",
    "            )\n",
    "            audios, images, videos = process_mm_info(\n",
    "                conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "            )\n",
    "            inputs = processor(\n",
    "                text=text,\n",
    "                audio=audios,\n",
    "                images=images,\n",
    "                videos=videos,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                use_audio_in_video=USE_AUDIO_IN_VIDEO,\n",
    "            ).to(model.device).to(model.dtype)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Processor/prep failed for batch starting at sec %d: %s\", batch[0][\"sec\"], e)\n",
    "            for dbg in batch:\n",
    "                logger.error(\"   video_uri=%s | exists=%s\",\n",
    "                             dbg[\"video_uri\"], Path(dbg[\"video_path\"]).exists())\n",
    "            failed_seconds.extend([it[\"sec\"] for it in batch])\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.thinker(**inputs, output_hidden_states=True, return_dict=True)\n",
    "                last_hidden = outputs.hidden_states[-1]\n",
    "                pooled = pooled_last_hidden_state(last_hidden, inputs[\"attention_mask\"])\n",
    "\n",
    "                embeds_all.append(pooled.detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            logger.error(\"Model forward failed for batch starting at sec %d: %s\", batch[0][\"sec\"], e)\n",
    "            failed_seconds.extend([it[\"sec\"] for it in batch])\n",
    "            continue\n",
    "\n",
    "    if not embeds_all:\n",
    "        raise RuntimeError(\"No embeddings produced; all batches failed.\")\n",
    "\n",
    "    embeds = np.concatenate(embeds_all, axis=0)\n",
    "    if failed_seconds:\n",
    "        logger.warning(\"Failed seconds (skipped): %s\", failed_seconds)\n",
    "    else:\n",
    "        logger.info(\"All seconds processed successfully.\")\n",
    "    return embeds\n",
    "\n",
    "# ----------------------------\n",
    "# Entry point\n",
    "# ----------------------------\n",
    "def main():\n",
    "    start_total = time.time()\n",
    "\n",
    "    # ---- Load model ----\n",
    "    t0 = time.time()\n",
    "    model, processor = load_qwen()\n",
    "    logger.info(\"Model + processor loaded in %.2f sec\", time.time() - t0)\n",
    "\n",
    "    # ---- Slice video ----\n",
    "    t1 = time.time()\n",
    "    try:\n",
    "        if PREFER_FFMPEG_SLICER and ffmpeg_available():\n",
    "            sec_items = slice_video_ffmpeg(VIDEO_PATH, OUT_DIR, max_secs=MAX_SECS)\n",
    "        else:\n",
    "            logger.warning(\"Using MoviePy slicer (FFmpeg unavailable or preference disabled).\")\n",
    "            sec_items = slice_video_v2(VIDEO_PATH, OUT_DIR, max_secs=MAX_SECS)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Slicing failed.\")\n",
    "        raise SystemExit(e)\n",
    "    logger.info(\"Video slicing completed in %.2f sec\", time.time() - t1)\n",
    "\n",
    "    # ---- Extract embeddings ----\n",
    "    t2 = time.time()\n",
    "    try:\n",
    "        embeds = extract_second_level_embeddings(model, processor, sec_items)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Embedding extraction failed.\")\n",
    "        raise SystemExit(e)\n",
    "    logger.info(\"Embedding extraction completed in %.2f sec\", time.time() - t2)\n",
    "\n",
    "    # ---- Save embeddings ----\n",
    "    np.save(SAVE_EMBEDDINGS_NPY, embeds)\n",
    "    logger.info(\"Saved embeddings: shape=%s -> %s\", embeds.shape, SAVE_EMBEDDINGS_NPY)\n",
    "\n",
    "    # ---- Total time ----\n",
    "    total = time.time() - start_total\n",
    "    logger.info(\"Total runtime: %.2f sec (%.2f min)\", total, total / 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0248ff6-0b5c-457c-9038-597bdbc01344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the .npy file\n",
    "data = np.load('./second_level_embeddings.npy')\n",
    "\n",
    "# Print the first line (first element) of the array\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb74be-5996-4b46-9db8-e39b6b169eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the embeddings from the .npy file\n",
    "embeddings = np.load('./second_level_embeddings.npy')\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Display the shape of the similarity matrix\n",
    "print(\"Similarity matrix shape:\", similarity_matrix.shape)\n",
    "\n",
    "# Optionally, print a portion of the matrix\n",
    "print(\"Sample similarity values:\\n\", similarity_matrix[:20, :20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb2a36-f785-4668-95b0-325bb7804908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load('./second_level_embeddings.npy')\n",
    "\n",
    "# Compute cosine similarity between each consecutive pair\n",
    "similarities = []\n",
    "for i in range(len(embeddings) - 1):\n",
    "    sim = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
    "    similarities.append(sim)\n",
    "\n",
    "# Print the results\n",
    "for idx, value in enumerate(similarities):\n",
    "    print(f\"Similarity between vector {idx} and {idx+1}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f35901-0af7-4654-ab7d-e271af60c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########code for audio transcript###########\n",
    "\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "import wave\n",
    "import json\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "# Optional: reduce Vosk logging\n",
    "SetLogLevel(-1)\n",
    "\n",
    "# 1. Extract audio from video\n",
    "video = VideoFileClip(\"/storage/home/saichandc/video/90secvideo.mp4\")\n",
    "video.audio.write_audiofile(\"audio.wav\", codec='pcm_s16le', fps=16000)\n",
    "\n",
    "# 2. Load Vosk model with word timestamps enabled\n",
    "model = Model(\"/storage/home/saichandc/qwen/vosk\")\n",
    "wf = wave.open(\"audio.wav\", \"rb\")\n",
    "\n",
    "# Important: Set words=True for word-level timestamps\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "rec.SetWords(True)  # Enable word-level timestamps\n",
    "\n",
    "# 3. Process audio and collect word timestamps\n",
    "word_timestamps = []\n",
    "\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    \n",
    "    if rec.AcceptWaveform(data):\n",
    "        result = json.loads(rec.Result())\n",
    "        if 'result' in result:\n",
    "            word_timestamps.extend(result['result'])\n",
    "\n",
    "# Don't forget the final result\n",
    "final_result = json.loads(rec.FinalResult())\n",
    "if 'result' in final_result:\n",
    "    word_timestamps.extend(final_result['result'])\n",
    "\n",
    "# 4. Display word-by-word with timestamps\n",
    "for word_info in word_timestamps:\n",
    "    word = word_info['word']\n",
    "    start = word_info['start']  # in seconds\n",
    "    end = word_info['end']      # in seconds\n",
    "    print(f\"{start:.2f}s - {end:.2f}s: {word}\")\n",
    "\n",
    "# 5. Optional: Create second-by-second transcript\n",
    "def get_transcript_by_second(word_timestamps, video_duration):\n",
    "    \"\"\"Group words by second\"\"\"\n",
    "    transcript_by_second = {}\n",
    "    \n",
    "    for word_info in word_timestamps:\n",
    "        word = word_info['word']\n",
    "        start = int(word_info['start'])\n",
    "        \n",
    "        if start not in transcript_by_second:\n",
    "            transcript_by_second[start] = []\n",
    "        transcript_by_second[start].append(word)\n",
    "    \n",
    "    # Format output\n",
    "    for second in range(int(video_duration) + 1):\n",
    "        words = transcript_by_second.get(second, [])\n",
    "        text = \" \".join(words) if words else \"[silence]\"\n",
    "        print(f\"Second {second}: {text}\")\n",
    "    \n",
    "    return transcript_by_second\n",
    "\n",
    "# Get video duration\n",
    "video_duration = video.duration\n",
    "transcript = get_transcript_by_second(word_timestamps, video_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a322743-f427-4f0c-a555-008cf8cc9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2.88G/2.88G [00:27<00:00, 113MiB/s]\n",
      "/storage/home/saichandc/qwen/qwen_env_311/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second 0: Until February. All right. There\n",
      "Second 1: aren't 100 million people\n",
      "Second 2: with preexisting conditions.\n",
      "Second 3: As\n",
      "Second 4: far as the say\n",
      "Second 5: is concerned, the people\n",
      "Second 6: already had their say.\n",
      "Second 7: Okay.\n",
      "Second 8: [silence]\n",
      "Second 9: Justice Ginsburg\n",
      "Second 10: said very\n",
      "Second 11: powerfully, very strongly,\n",
      "Second 12: at\n",
      "Second 13: some point, 10\n",
      "Second 14: years ago\n",
      "Second 15: or so, she\n",
      "Second 16: said a\n",
      "Second 17: president and the Senate\n",
      "Second 18: is elected\n",
      "Second 19: for a period\n",
      "Second 20: of time. But a president's\n",
      "Second 21: elected for four\n",
      "Second 22: years. We're not elected\n",
      "Second 23: for three years. I'm\n",
      "Second 24: not elected for three years.\n",
      "Second 25: So we\n",
      "Second 26: have the Senate. We\n",
      "Second 27: have a president. He's elected\n",
      "Second 28: to the next election. During\n",
      "Second 29: that period of time,\n",
      "Second 30: during that\n",
      "Second 31: period of time, we\n",
      "Second 32: have an\n",
      "Second 33: opening. I'm\n",
      "Second 34: not elected for three\n",
      "Second 35: years. I'm elected for\n",
      "Second 36: four years. And\n",
      "Second 37: the 100 million\n",
      "Second 38: people, Joe, the\n",
      "Second 39: 100 million people\n",
      "Second 40: is totally wrong.\n",
      "Second 41: I don't know where you got that number.\n",
      "Second 42: The bigger\n",
      "Second 43: problem that you have\n",
      "Second 44: is that\n",
      "Second 45: you're going to extinguish\n",
      "Second 46: 180\n",
      "Second 47: million people\n",
      "Second 48: with their private\n",
      "Second 49: health care that\n",
      "Second 50: they're very happy with.\n",
      "Second 51: That's simply not true. Well, you're\n",
      "Second 52: certainly going to socialist.\n",
      "Second 53: You're going to socialist\n",
      "Second 54: medicine. Gentlemen,\n",
      "Second 55: [silence]\n",
      "Second 56: we're now into open discussion.\n",
      "Second 57: Open discussion.\n",
      "Second 58: Open discussion. Yes, I agree.\n",
      "Second 59: Go ahead, Vice\n",
      "Second 60: President. Number one, he\n",
      "Second 61: knows\n",
      "Second 62: that what\n",
      "Second 63: I proposed.\n",
      "Second 64: What I proposed\n",
      "Second 65: is that we\n",
      "Second 66: expand\n",
      "Second 67: Obamacare\n",
      "Second 68: and we\n",
      "Second 69: increase it. We do\n",
      "Second 70: not wipe any.\n",
      "Second 71: And one of the\n",
      "Second 72: big debates we had\n",
      "Second 73: with 23 of\n",
      "Second 74: my colleagues\n",
      "Second 75: trying to win the\n",
      "Second 76: nomination that I won.\n",
      "Second 77: We're saying\n",
      "Second 78: that Biden wanted\n",
      "Second 79: to allow people\n",
      "Second 80: to have private insurance still.\n",
      "Second 81: They\n",
      "Second 82: can. They\n",
      "Second 83: do. They will\n",
      "Second 84: under my proposal.\n",
      "Second 85: It's not what you said. And\n",
      "Second 86: it's not what your party has\n",
      "Second 87: said. That is simply a\n",
      "Second 88: lie. Your party doesn't say it.\n",
      "Second 89: Your party wants to go\n",
      "Second 90: socialist medicine.\n",
      "Second 91: My party is\n",
      "Second 92: me. And socialist health\n",
      "Second 93: care. Right now, I am\n",
      "Second 94: the Democratic Party. And they're going to dominate you,\n",
      "Second 95: Joe. You know that.\n",
      "Second 96: I am the Democratic Party\n",
      "Second 97: right now.\n",
      "Second 98: The platform.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "def get_transcript_by_second(video_path):\n",
    "    # Load model\n",
    "    model = whisper.load_model(\"large\")\n",
    "    \n",
    "    # Transcribe with word timestamps\n",
    "    result = model.transcribe(video_path, word_timestamps=True)\n",
    "    \n",
    "    # Organize by second\n",
    "    transcript_by_second = {}\n",
    "    \n",
    "    for segment in result[\"segments\"]:\n",
    "        if \"words\" in segment:\n",
    "            for word_info in segment[\"words\"]:\n",
    "                second = int(word_info[\"start\"])\n",
    "                word = word_info[\"word\"].strip()\n",
    "                \n",
    "                if second not in transcript_by_second:\n",
    "                    transcript_by_second[second] = []\n",
    "                transcript_by_second[second].append(word)\n",
    "    \n",
    "    # Print second-by-second\n",
    "    max_second = max(transcript_by_second.keys()) if transcript_by_second else 0\n",
    "    for second in range(max_second + 1):\n",
    "        words = transcript_by_second.get(second, [])\n",
    "        text = \" \".join(words) if words else \"[silence]\"\n",
    "        print(f\"Second {second}: {text}\")\n",
    "    \n",
    "    return transcript_by_second\n",
    "\n",
    "# Use it\n",
    "transcript = get_transcript_by_second(\"/storage/home/saichandc/video/90secvideo.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8b3ebd-1556-47fd-aebc-e0c5e561ae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:25:58\n"
     ]
    }
   ],
   "source": [
    "print(__import__('datetime').datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89826ccd-de14-4be1-8bf5-a0e7b4b3be6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2289724161.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython samplePython.py\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "########script for PBS ########\n",
    "\n",
    "#!/bin/bash\n",
    "#PBS -N pythonjob\n",
    "#PBS -e error.log\n",
    "#PBS -l select=2:ncpus=16\n",
    "#PBS -q GPU\n",
    "#PBS -l walltime=24:00:00\n",
    "module load python/3.11.4\n",
    "\n",
    "cd /storage/home/saichandc/qwen\n",
    "source qwen_env_311/bin/activate\n",
    "\n",
    "python whispertranscript.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60162e9e-c4f9-422a-8d6e-f9e5062dc8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
